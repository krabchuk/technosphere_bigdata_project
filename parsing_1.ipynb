{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sleep(offset=1.5, length=4):\n",
    "    sleep(random.random() * length + offset)\n",
    "\n",
    "def get_html(url):\n",
    "    try:\n",
    "        random_sleep()\n",
    "        r = requests.get(url)\n",
    "    except:\n",
    "        random_sleep(1000, 500)\n",
    "        r = requests.get(url)\n",
    "        return r.text\n",
    "    return r.text\n",
    "\n",
    "# получаем ссылки на все страницы\n",
    "def get_total_pages(html, x=0):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    last_page_html = soup.find_all('a', class_='pagination-page')[-1].get('href')\n",
    "    num_last_page = last_page_html.split('=')[1].split('&')[0]\n",
    "    \n",
    "    all_urls = ['https://www.avito.ru' + last_page_html.split('=')[0] + '=' \\\n",
    "                + str(i) + '&radius=0&f=188_893b0&i=1' \\\n",
    "                for i in range(x+1, int(num_last_page))]\n",
    "    return all_urls\n",
    "    \n",
    "# получаем все ссылки на обхъявления с одной страницы\n",
    "def get_page_urls(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    sp_ads = soup.find_all('a', class_='item-description-title-link')\n",
    "    sleep(1)\n",
    "    ads = [i.get('href') for i in sp_ads]\n",
    "    urls = ['https://www.avito.ru' + str(i) for i in ads]\n",
    "    return urls\n",
    "\n",
    "# получаем данные одного объявления\n",
    "def get_page_data(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    params_html = soup.find('div', class_='item-params').find_all('li', class_='item-params-list-item')\n",
    "    \n",
    "    params = [i.text.strip() for i in params_html]\n",
    "    price_html = soup.find('span', class_='js-item-price')\n",
    "    price = 'Цена: ' + price_html.text.strip()\n",
    "    params.append(price)\n",
    "    return params\n",
    "\n",
    "# получаем все даныне об объявлениях одной страниы\n",
    "def get_all_page_params(html):\n",
    "    urls = get_page_urls(html)\n",
    "    parameters = []\n",
    "    for url in urls:\n",
    "        html = get_html(url)\n",
    "        param = get_page_data(html)\n",
    "        parameters.append(param)\n",
    "    return parameters\n",
    "\n",
    "# надо вставить url 2ой страницы из авито\n",
    "# получаем массив из строк для всех страниц всех обхявлений\n",
    "def get_all_data(url, x=0, num=4000):\n",
    "    html_for_total_pages = get_html(url)\n",
    "    total_pages_urls = get_total_pages(html_for_total_pages, x)\n",
    "    sleep(10)\n",
    "    all_data = []\n",
    "    \n",
    "    num_ads = 0 # считать сколько обхявлений скачалось\n",
    "    num_pages = 0\n",
    "    for page_url in total_pages_urls:\n",
    "        try:\n",
    "            page_html = get_html(page_url)\n",
    "            adds_urls = get_page_urls(page_html)\n",
    "        except:\n",
    "            print(\"объявлений/страниц: \", num_ads,\" \", num_pages)\n",
    "            return all_data\n",
    "        num_pages += 1\n",
    "        \n",
    "        page_data = []\n",
    "\n",
    "        random_sleep(25, 10)\n",
    "        for ad_url in adds_urls:\n",
    "            try:\n",
    "                ad_html = get_html(ad_url)\n",
    "                random_sleep(8, 8)\n",
    "                one_data = get_page_data(ad_html)\n",
    "            except:\n",
    "                print(\"объявлений/страниц: \", num_ads,\" \", num_pages)\n",
    "                all_data.extend(page_data)\n",
    "                return all_data\n",
    "            num_ads += 1\n",
    "            if (num_ads == num):\n",
    "                print(\"объявлений/страниц: \", num_ads,\" \", num_pages)\n",
    "                all_data.extend(page_data)\n",
    "                return all_data\n",
    "            \n",
    "            if (num_ads % 50 == 0):\n",
    "                random_sleep(10, 20)\n",
    "            if (num_ads % 200 == 0):\n",
    "                random_sleep(20,40)\n",
    "                \n",
    "            url_of_data = 'Ссылка: ' + ad_url\n",
    "            one_data.append(url_of_data)\n",
    "            page_data.append(one_data)\n",
    "        all_data.extend(page_data)\n",
    "        \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем из массива массивов строк dataframe\n",
    "def get_pd_data(data):\n",
    "    df = pd.DataFrame(index=np.arange(len(data)), columns=['Марка', 'Модель', 'Год выпуска', 'Тип кузова',\n",
    "                                                   'Цена', 'Объём двигателя', 'Цвет', 'Привод',\n",
    "                                                   'Руль', 'Тип двигателя', 'Состояние', 'Владельцев по ПТС',\n",
    "                                                   'Количество дверей', 'Мощность двигателя', 'Пробег',\n",
    "                                                   'VIN или номер кузова', 'Коробка передач', 'Ссылка'])\n",
    "    for j in range(len(data)):\n",
    "        for i in range(len(data[j])):\n",
    "            tmp = data[j][i].split(': ')\n",
    "            df.loc[j][tmp[0]] = tmp[1]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.avito.ru/moskva/avtomobili/ford?p=2&radius=0&f=188_893b0&i=1'\n",
    "list_of_data = get_all_data(url)\n",
    "df = get_pd_data(list_of_data)\n",
    "df.to_csv('ford1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sleep(300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.avito.ru/moskva/avtomobili/hyundai?p=2&radius=0&f=188_893b0&i=1'\n",
    "list_of_data = get_all_data(url)\n",
    "df = get_pd_data(list_of_data)\n",
    "df.to_csv('hyundai1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sleep(300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.avito.ru/moskva/avtomobili/mercedes-benz?p=2&radius=0&f=188_893b0&i=1'\n",
    "list_of_data = get_all_data(url)\n",
    "df = get_pd_data(list_of_data)\n",
    "df.to_csv('mercedes-benz1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.avito.ru/moskva/avtomobili/volkswagen?p=2&radius=0&f=188_893b0&i=1'\n",
    "list_of_data = get_all_data(url)\n",
    "df = get_pd_data(list_of_data)\n",
    "df.to_csv('volkswagen1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
